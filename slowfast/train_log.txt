config files: ['configs/MVITv2_B_32x3_prompted.yaml', 'DATA.PATH_TO_DATA_DIR', '.']
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
[07/13 10:06:34][INFO] train_net.py:  650: Train with config:
[07/13 10:06:34][INFO] train_net.py:  651: {'AUG': {'AA_TYPE': 'rand-m9-mstd0.5-inc1',
         'COLOR_JITTER': 0.4,
         'ENABLE': False,
         'GEN_MASK_LOADER': False,
         'INTERPOLATION': 'bicubic',
         'MASK_FRAMES': False,
         'MASK_RATIO': 0.0,
         'MASK_TUBE': False,
         'MASK_WINDOW_SIZE': [8, 7, 7],
         'MAX_MASK_PATCHES_PER_BLOCK': None,
         'NUM_SAMPLE': 1,
         'RE_COUNT': 1,
         'RE_MODE': 'pixel',
         'RE_PROB': 0.25,
         'RE_SPLIT': False},
 'AVA': {'ANNOTATION_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/',
         'BGR': False,
         'DETECTION_SCORE_THRESH': 0.9,
         'EXCLUSION_FILE': 'ava_val_excluded_timestamps_v2.2.csv',
         'FRAME_DIR': '/mnt/fair-flash3-east/ava_trainval_frames.img/',
         'FRAME_LIST_DIR': '/mnt/vol/gfsai-flash3-east/ai-group/users/haoqifan/ava/frame_list/',
         'FULL_TEST_ON_VAL': False,
         'GROUNDTRUTH_FILE': 'ava_val_v2.2.csv',
         'IMG_PROC_BACKEND': 'cv2',
         'LABEL_MAP_FILE': 'ava_action_list_v2.2_for_activitynet_2019.pbtxt',
         'TEST_FORCE_FLIP': False,
         'TEST_LISTS': ['val.csv'],
         'TEST_PREDICT_BOX_LISTS': ['ava_val_predicted_boxes.csv'],
         'TRAIN_GT_BOX_LISTS': ['ava_train_v2.2.csv'],
         'TRAIN_LISTS': ['train.csv'],
         'TRAIN_PCA_JITTER_ONLY': True,
         'TRAIN_PREDICT_BOX_LISTS': [],
         'TRAIN_USE_COLOR_AUGMENTATION': False},
 'BENCHMARK': CfgNode({'NUM_EPOCHS': 5, 'LOG_PERIOD': 100, 'SHUFFLE': True}),
 'BN': {'GLOBAL_SYNC': False,
        'NORM_TYPE': 'batchnorm',
        'NUM_BATCHES_PRECISE': 200,
        'NUM_SPLITS': 1,
        'NUM_SYNC_DEVICES': 1,
        'USE_PRECISE_STATS': False,
        'WEIGHT_DECAY': 0.0},
 'CONTRASTIVE': {'BN_MLP': False,
                 'BN_SYNC_MLP': False,
                 'DELTA_CLIPS_MAX': inf,
                 'DELTA_CLIPS_MIN': -inf,
                 'DIM': 128,
                 'INTERP_MEMORY': False,
                 'KNN_ON': True,
                 'LENGTH': 239975,
                 'LOCAL_SHUFFLE_BN': True,
                 'MEM_TYPE': '1d',
                 'MLP_DIM': 2048,
                 'MOCO_MULTI_VIEW_QUEUE': False,
                 'MOMENTUM': 0.5,
                 'MOMENTUM_ANNEALING': False,
                 'NUM_CLASSES_DOWNSTREAM': 400,
                 'NUM_MLP_LAYERS': 1,
                 'PREDICTOR_DEPTHS': [],
                 'QUEUE_LEN': 65536,
                 'SEQUENTIAL': False,
                 'SIMCLR_DIST_ON': True,
                 'SWAV_QEUE_LEN': 0,
                 'T': 0.07,
                 'TYPE': 'mem'},
 'DATA': {'CAM_VIEWS_METHODS': ['crop', 'noise_crop'],
          'COLOR_RND_GRAYSCALE': 0.0,
          'CROP_PROMPT': False,
          'DECODING_BACKEND': 'torchvision',
          'DECODING_SHORT_SIZE': 256,
          'DUMMY_LOAD': False,
          'ENSEMBLE_METHOD': 'sum',
          'IN22K_TRAINVAL': False,
          'IN22k_VAL_IN1K': '',
          'INPUT_CHANNEL_NUM': [3],
          'INV_UNIFORM_SAMPLE': False,
          'IN_VAL_CROP_RATIO': 0.875,
          'LOADER_CHUNK_OVERALL_SIZE': 0,
          'LOADER_CHUNK_SIZE': 0,
          'MEAN': [0.45, 0.45, 0.45],
          'MULTI_LABEL': False,
          'NUM_FRAMES': 16,
          'PATH_LABEL_SEPARATOR': ' ',
          'PATH_PREFIX': '',
          'PATH_TO_DATA_DIR': '',
          'PATH_TO_PRELOAD_IMDB': '',
          'RANDOM_FLIP': False,
          'RETURN_CROPPING_PARAMS': False,
          'REVERSE_INPUT_CHANNEL': False,
          'SAMPLING_RATE': 4,
          'SKIP_ROWS': 0,
          'SSL_BLUR_SIGMA_MAX': [0.0, 2.0],
          'SSL_BLUR_SIGMA_MIN': [0.0, 0.1],
          'SSL_COLOR_BRI_CON_SAT': [0.4, 0.4, 0.4],
          'SSL_COLOR_HUE': 0.1,
          'SSL_COLOR_JITTER': False,
          'SSL_MOCOV2_AUG': False,
          'STD': [0.225, 0.225, 0.225],
          'TARGET_FPS': 30,
          'TEST_CROP_SIZE': 224,
          'TIME_DIFF_PROB': 0.0,
          'TRAIN_CROP_NUM_SPATIAL': 1,
          'TRAIN_CROP_NUM_TEMPORAL': 1,
          'TRAIN_CROP_SIZE': 224,
          'TRAIN_JITTER_ASPECT_RELATIVE': [0.75, 1.3333],
          'TRAIN_JITTER_FPS': 0.0,
          'TRAIN_JITTER_MOTION_SHIFT': False,
          'TRAIN_JITTER_SCALES': [256, 320],
          'TRAIN_JITTER_SCALES_RELATIVE': [0.08, 1.0],
          'TRAIN_PCA_EIGVAL': [0.225, 0.224, 0.229],
          'TRAIN_PCA_EIGVEC': [[-0.5675, 0.7192, 0.4009],
                               [-0.5808, -0.0045, -0.814],
                               [-0.5836, -0.6948, 0.4203]],
          'USE_OFFSET_SAMPLING': False},
 'DATA_LOADER': {'ENABLE_MULTI_THREAD_DECODE': False,
                 'NUM_WORKERS': 4,
                 'PIN_MEMORY': True},
 'DEMO': {'BUFFER_SIZE': 0,
          'CLIP_VIS_SIZE': 10,
          'COMMON_CLASS_NAMES': ['watch (a person)',
                                 'talk to (e.g., self, a person, a group)',
                                 'listen to (a person)',
                                 'touch (an object)',
                                 'carry/hold (an object)',
                                 'walk',
                                 'sit',
                                 'lie/sleep',
                                 'bend/bow (at the waist)'],
          'COMMON_CLASS_THRES': 0.7,
          'DETECTRON2_CFG': 'COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',
          'DETECTRON2_THRESH': 0.9,
          'DETECTRON2_WEIGHTS': 'detectron2://COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',
          'DISPLAY_HEIGHT': 0,
          'DISPLAY_WIDTH': 0,
          'ENABLE': False,
          'FPS': 30,
          'GT_BOXES': '',
          'INPUT_FORMAT': 'BGR',
          'INPUT_VIDEO': '',
          'LABEL_FILE_PATH': '',
          'NUM_CLIPS_SKIP': 0,
          'NUM_VIS_INSTANCES': 2,
          'OUTPUT_FILE': '',
          'OUTPUT_FPS': -1,
          'PREDS_BOXES': '',
          'SLOWMO': 1,
          'STARTING_SECOND': 900,
          'THREAD_ENABLE': False,
          'UNCOMMON_CLASS_THRES': 0.3,
          'VIS_MODE': 'thres',
          'WEBCAM': -1},
 'DETECTION': {'ALIGNED': True,
               'ENABLE': False,
               'ROI_XFORM_RESOLUTION': 7,
               'SPATIAL_SCALE_FACTOR': 16},
 'DIST_BACKEND': 'nccl',
 'LOG_MODEL_INFO': True,
 'LOG_PERIOD': 10,
 'MASK': {'DECODER_DEPTH': 0,
          'DECODER_EMBED_DIM': 512,
          'DECODER_SEP_POS_EMBED': False,
          'DEC_KV_KERNEL': [],
          'DEC_KV_STRIDE': [],
          'ENABLE': False,
          'HEAD_TYPE': 'separate',
          'MAE_ON': False,
          'MAE_RND_MASK': False,
          'NORM_PRED_PIXEL': True,
          'PER_FRAME_MASKING': False,
          'PRED_HOG': False,
          'PRETRAIN_DEPTH': [15],
          'SCALE_INIT_BY_DEPTH': False,
          'TIME_STRIDE_LOSS': True},
 'MIXUP': {'ALPHA': 0.8,
           'CUTMIX_ALPHA': 1.0,
           'ENABLE': False,
           'LABEL_SMOOTH_VALUE': 0.1,
           'PROB': 1.0,
           'SWITCH_PROB': 0.5},
 'MODEL': {'ACT_CHECKPOINT': False,
           'ARCH': 'mvit',
           'DETACH_FINAL_FC': False,
           'DROPCONNECT_RATE': 0.0,
           'DROPOUT_RATE': 0.5,
           'FC_INIT_STD': 0.01,
           'FP16_ALLREDUCE': False,
           'FROZEN_BN': False,
           'HEAD_ACT': 'softmax',
           'LOSS_FUNC': 'soft_cross_entropy',
           'MODEL_NAME': 'MViT',
           'MULTI_PATHWAY_ARCH': ['slowfast'],
           'NUM_CLASSES': 16,
           'SINGLE_PATHWAY_ARCH': ['2d',
                                   'c2d',
                                   'i3d',
                                   'slow',
                                   'x3d',
                                   'mvit',
                                   'maskmvit']},
 'MULTIGRID': {'BN_BASE_SIZE': 8,
               'DEFAULT_B': 0,
               'DEFAULT_S': 0,
               'DEFAULT_T': 0,
               'EPOCH_FACTOR': 1.5,
               'EVAL_FREQ': 3,
               'LONG_CYCLE': False,
               'LONG_CYCLE_FACTORS': [(0.25, 0.7071067811865476),
                                      (0.5, 0.7071067811865476),
                                      (0.5, 1),
                                      (1, 1)],
               'LONG_CYCLE_SAMPLING_RATE': 0,
               'SHORT_CYCLE': False,
               'SHORT_CYCLE_FACTORS': [0.5, 0.7071067811865476]},
 'MVIT': {'CLS_EMBED_ON': True,
          'DEPTH': 24,
          'DIM_MUL': [[2, 2.0], [5, 2.0], [21, 2.0]],
          'DIM_MUL_IN_ATT': True,
          'DROPOUT_RATE': 0.0,
          'DROPPATH_RATE': 0.3,
          'EMBED_DIM': 96,
          'HEAD_INIT_SCALE': 1.0,
          'HEAD_MUL': [[2, 2.0], [5, 2.0], [21, 2.0]],
          'LAYER_SCALE_INIT_VALUE': 0.0,
          'MLP_RATIO': 4.0,
          'MODE': 'conv',
          'NORM': 'layernorm',
          'NORM_STEM': False,
          'NUM_HEADS': 1,
          'PATCH_2D': False,
          'PATCH_KERNEL': [3, 7, 7],
          'PATCH_PADDING': [1, 3, 3],
          'PATCH_STRIDE': [2, 4, 4],
          'POOL_FIRST': False,
          'POOL_KVQ_KERNEL': [3, 3, 3],
          'POOL_KV_STRIDE': [],
          'POOL_KV_STRIDE_ADAPTIVE': [1, 8, 8],
          'POOL_Q_STRIDE': [[0, 1, 1, 1],
                            [1, 1, 1, 1],
                            [2, 1, 2, 2],
                            [3, 1, 1, 1],
                            [4, 1, 1, 1],
                            [5, 1, 2, 2],
                            [6, 1, 1, 1],
                            [7, 1, 1, 1],
                            [8, 1, 1, 1],
                            [9, 1, 1, 1],
                            [10, 1, 1, 1],
                            [11, 1, 1, 1],
                            [12, 1, 1, 1],
                            [13, 1, 1, 1],
                            [14, 1, 1, 1],
                            [15, 1, 1, 1],
                            [16, 1, 1, 1],
                            [17, 1, 1, 1],
                            [18, 1, 1, 1],
                            [19, 1, 1, 1],
                            [20, 1, 1, 1],
                            [21, 1, 2, 2],
                            [22, 1, 1, 1],
                            [23, 1, 1, 1]],
          'QKV_BIAS': True,
          'REL_POS_SPATIAL': True,
          'REL_POS_TEMPORAL': True,
          'REL_POS_ZERO_INIT': False,
          'RESIDUAL_POOLING': True,
          'REV': {'BUFFER_LAYERS': [],
                  'ENABLE': False,
                  'PRE_Q_FUSION': 'avg',
                  'RESPATH_FUSE': 'concat',
                  'RES_PATH': 'conv'},
          'SEPARATE_QKV': False,
          'SEP_POS_EMBED': False,
          'USE_ABS_POS': False,
          'USE_FIXED_SINCOS_POS': False,
          'USE_MEAN_POOLING': False,
          'ZERO_DECAY_POS_CLS': False},
 'NONLOCAL': {'GROUP': [[1], [1], [1], [1]],
              'INSTANTIATION': 'dot_product',
              'LOCATION': [[[]], [[]], [[]], [[]]],
              'POOL': [[[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]],
                       [[1, 2, 2], [1, 2, 2]]]},
 'NUM_GPUS': 2,
 'NUM_SHARDS': 1,
 'OUTPUT_DIR': '.',
 'PROMPT': {'ENABLE': True,
            'GPU': None,
            'IMAGE_FOLDER': './visual_prompting/save/images/mvitv2-b_fixed_patch',
            'LEARNING_RATE': 0.1,
            'METHOD': 'multi_cam_noisecropv2',
            'MODEL_FOLDER': './visual_prompting/save/models/mvitv2-b_fixed_patch',
            'MOMENTUM': 0.9,
            'PRINT_GRADS': False,
            'PROMPT_SIZE': 30,
            'RESUME': None,
            'SELECTIVE_UPDATING': True,
            'START_EPOCH': 1,
            'WARMUP': 30,
            'WEIGHT_DECAY': 0.0001},
 'RESNET': {'DEPTH': 50,
            'INPLACE_RELU': True,
            'NUM_BLOCK_TEMP_KERNEL': [[3], [4], [6], [3]],
            'NUM_GROUPS': 1,
            'SPATIAL_DILATIONS': [[1], [1], [1], [1]],
            'SPATIAL_STRIDES': [[1], [2], [2], [2]],
            'STRIDE_1X1': False,
            'TRANS_FUNC': 'bottleneck_transform',
            'WIDTH_PER_GROUP': 64,
            'ZERO_INIT_FINAL_BN': False,
            'ZERO_INIT_FINAL_CONV': False},
 'RNG_SEED': 0,
 'SHARD_ID': 0,
 'SLOWFAST': {'ALPHA': 8,
              'BETA_INV': 8,
              'FUSION_CONV_CHANNEL_RATIO': 2,
              'FUSION_KERNEL_SZ': 5},
 'SOLVER': {'BASE_LR': 0.0002,
            'BASE_LR_SCALE_NUM_SHARDS': True,
            'BETAS': (0.9, 0.999),
            'CLIP_GRAD_L2NORM': 1.0,
            'CLIP_GRAD_VAL': None,
            'COSINE_AFTER_WARMUP': True,
            'COSINE_END_LR': 2e-06,
            'DAMPENING': 0.0,
            'GAMMA': 0.1,
            'LARS_ON': False,
            'LAYER_DECAY': 1.0,
            'LRS': [],
            'LR_POLICY': 'cosine',
            'MAX_EPOCH': 200,
            'MOMENTUM': 0.9,
            'NESTEROV': True,
            'OPTIMIZING_METHOD': 'adamw',
            'STEPS': [],
            'STEP_SIZE': 1,
            'WARMUP_EPOCHS': 30.0,
            'WARMUP_FACTOR': 0.1,
            'WARMUP_START_LR': 2e-06,
            'WEIGHT_DECAY': 0.0001,
            'ZERO_WD_1D_PARAM': True},
 'TASK': '',
 'TENSORBOARD': {'CATEGORIES_PATH': '',
                 'CLASS_NAMES_PATH': '',
                 'CONFUSION_MATRIX': {'ENABLE': False,
                                      'FIGSIZE': [8, 8],
                                      'SUBSET_PATH': ''},
                 'ENABLE': False,
                 'HISTOGRAM': {'ENABLE': False,
                               'FIGSIZE': [8, 8],
                               'SUBSET_PATH': '',
                               'TOPK': 10},
                 'LOG_DIR': '',
                 'MODEL_VIS': {'ACTIVATIONS': False,
                               'COLORMAP': 'Pastel2',
                               'ENABLE': False,
                               'GRAD_CAM': {'COLORMAP': 'viridis',
                                            'ENABLE': True,
                                            'LAYER_LIST': [],
                                            'USE_TRUE_LABEL': False},
                               'INPUT_VIDEO': False,
                               'LAYER_LIST': [],
                               'MODEL_WEIGHTS': False,
                               'TOPK_PREDS': 1},
                 'PREDICTIONS_PATH': '',
                 'WRONG_PRED_VIS': {'ENABLE': False,
                                    'SUBSET_PATH': '',
                                    'TAG': 'Incorrectly classified videos.'}},
 'TEST': {'BATCH_SIZE': 2,
          'CHECKPOINT_FILE_PATH': '',
          'CHECKPOINT_TYPE': 'pytorch',
          'DATASET': 'kinetics',
          'ENABLE': False,
          'NUM_ENSEMBLE_VIEWS': 5,
          'NUM_SPATIAL_CROPS': 1,
          'NUM_TEMPORAL_CLIPS': [],
          'SAVE_RESULTS_PATH': ''},
 'TRAIN': {'AUTO_RESUME': True,
           'BATCH_SIZE': 16,
           'CHECKPOINT_CLEAR_NAME_PATTERN': (),
           'CHECKPOINT_EPOCH_RESET': True,
           'CHECKPOINT_FILE_PATH': 'checkpoints/MViTv2_B_32x3_k400_f304025456.pyth',
           'CHECKPOINT_INFLATE': False,
           'CHECKPOINT_IN_INIT': False,
           'CHECKPOINT_PERIOD': 20,
           'CHECKPOINT_TYPE': 'pytorch',
           'DATASET': 'kinetics',
           'ENABLE': True,
           'EVAL_PERIOD': 2,
           'KILL_LOSS_EXPLOSION_FACTOR': 0.0,
           'MIXED_PRECISION': False},
 'VIS_MASK': CfgNode({'ENABLE': False}),
 'X3D': {'BN_LIN5': False,
         'BOTTLENECK_FACTOR': 1.0,
         'CHANNELWISE_3x3x3': True,
         'DEPTH_FACTOR': 1.0,
         'DIM_C1': 12,
         'DIM_C5': 2048,
         'SCALE_RES2': False,
         'WIDTH_FACTOR': 1.0}}
[07/13 10:06:36][INFO] misc.py:  185: Model:
DistributedDataParallel(
  (module): MViT(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 96, kernel_size=(3, 7, 7), stride=(2, 4, 4), padding=(1, 3, 3))
    )
    (blocks): ModuleList(
      (0): MultiScaleBlock(
        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 8, 8), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 8, 8), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=384, out_features=96, bias=True)
        )
      )
      (1): MultiScaleBlock(
        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=96, out_features=288, bias=True)
          (proj): Linear(in_features=96, out_features=96, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 8, 8), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 8, 8), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=96, out_features=384, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=384, out_features=96, bias=True)
        )
      )
      (2): MultiScaleBlock(
        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=96, out_features=576, bias=True)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
        (proj): Linear(in_features=96, out_features=192, bias=True)
        (pool_skip): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)
      )
      (3): MultiScaleBlock(
        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (4): MultiScaleBlock(
        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=192, out_features=576, bias=True)
          (proj): Linear(in_features=192, out_features=192, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 4, 4), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=192, out_features=768, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=768, out_features=192, bias=True)
        )
      )
      (5): MultiScaleBlock(
        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=192, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
        (proj): Linear(in_features=192, out_features=384, bias=True)
        (pool_skip): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)
      )
      (6): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (7): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (8): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (9): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (10): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (11): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (12): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (13): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (14): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (15): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (16): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (17): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (18): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (19): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (20): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
        )
      )
      (21): MultiScaleBlock(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=384, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 2, 2), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
        )
        (proj): Linear(in_features=384, out_features=768, bias=True)
        (pool_skip): MaxPool3d(kernel_size=[1, 3, 3], stride=[1, 2, 2], padding=[0, 1, 1], dilation=1, ceil_mode=False)
      )
      (22): MultiScaleBlock(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
        )
      )
      (23): MultiScaleBlock(
        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (attn): MultiScaleAttention(
          (qkv): Linear(in_features=768, out_features=2304, bias=True)
          (proj): Linear(in_features=768, out_features=768, bias=True)
          (pool_q): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_q): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_k): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_k): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
          (pool_v): Conv3d(96, 96, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=96, bias=False)
          (norm_v): LayerNorm((96,), eps=1e-06, elementwise_affine=True)
        )
        (drop_path): DropPath()
        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=768, out_features=3072, bias=True)
          (act): GELU(approximate='none')
          (fc2): Linear(in_features=3072, out_features=768, bias=True)
        )
      )
    )
    (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
    (head): TransformerBasicHead(
      (dropout): Dropout(p=0.5, inplace=False)
      (projection): Linear(in_features=768, out_features=16, bias=True)
      (act): Softmax(dim=1)
    )
  )
)
[07/13 10:06:36][INFO] misc.py:  187: Params: 50,897,968
[07/13 10:06:36][INFO] misc.py:  188: Mem: 0.38045549392700195 MB
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 729 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::div encountered 102 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::sub encountered 144 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::add_ encountered 96 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 120 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 25 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::gelu encountered 24 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  499: Unsupported operator aten::max_pool3d encountered 3 time(s)
[07/13 10:06:40][WARNING] jit_analysis.py:  511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
module.blocks.1.drop_path, module.blocks.10.drop_path, module.blocks.11.drop_path, module.blocks.12.drop_path, module.blocks.13.drop_path, module.blocks.14.drop_path, module.blocks.15.drop_path, module.blocks.16.drop_path, module.blocks.17.drop_path, module.blocks.18.drop_path, module.blocks.19.drop_path, module.blocks.2.drop_path, module.blocks.20.drop_path, module.blocks.21.drop_path, module.blocks.22.drop_path, module.blocks.23.drop_path, module.blocks.3.drop_path, module.blocks.4.drop_path, module.blocks.5.drop_path, module.blocks.6.drop_path, module.blocks.7.drop_path, module.blocks.8.drop_path, module.blocks.9.drop_path
[07/13 10:06:40][INFO] misc.py:  190: Flops: 92.63208983999999 G
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::layer_norm encountered 121 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::mul encountered 729 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::div encountered 102 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::sub encountered 144 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::add_ encountered 96 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::add encountered 120 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::softmax encountered 25 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::gelu encountered 24 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  499: Unsupported operator aten::max_pool3d encountered 3 time(s)
[07/13 10:06:43][WARNING] jit_analysis.py:  511: The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.
module.blocks.1.drop_path, module.blocks.10.drop_path, module.blocks.11.drop_path, module.blocks.12.drop_path, module.blocks.13.drop_path, module.blocks.14.drop_path, module.blocks.15.drop_path, module.blocks.16.drop_path, module.blocks.17.drop_path, module.blocks.18.drop_path, module.blocks.19.drop_path, module.blocks.2.drop_path, module.blocks.20.drop_path, module.blocks.21.drop_path, module.blocks.22.drop_path, module.blocks.23.drop_path, module.blocks.3.drop_path, module.blocks.4.drop_path, module.blocks.5.drop_path, module.blocks.6.drop_path, module.blocks.7.drop_path, module.blocks.8.drop_path, module.blocks.9.drop_path
[07/13 10:06:43][INFO] misc.py:  191: Activations: 357.084096 M
[07/13 10:06:43][INFO] misc.py:  196: nvidia-smi
Thu Jul 13 10:06:43 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA RTX A5000    Off  | 00000000:17:00.0 Off |                  Off |
| 30%   43C    P2    63W / 230W |   4179MiB / 24256MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  NVIDIA RTX A5000    Off  | 00000000:65:00.0 Off |                  Off |
| 30%   46C    P2    66W / 230W |   1371MiB / 24247MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1550      G   /usr/lib/xorg/Xorg                  4MiB |
|    0   N/A  N/A   4101148      C   /usr/bin/python3                 4171MiB |
|    1   N/A  N/A      1550      G   /usr/lib/xorg/Xorg                 32MiB |
|    1   N/A  N/A      2487      G   /usr/bin/gnome-shell                7MiB |
|    1   N/A  N/A   4101149      C   /usr/bin/python3                 1327MiB |
+-----------------------------------------------------------------------------+
bn 0, non bn 246, zero 343, no grad 0
[07/13 10:06:43][INFO] train_net.py:  692: Load from given checkpoint file.
[07/13 10:06:43][INFO] checkpoint.py:  223: Loading network weights from checkpoints/MViTv2_B_32x3_k400_f304025456.pyth.
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.0.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.1.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.2.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.3.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.4.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.5.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.6.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.7.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.8.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.9.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.10.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.11.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.12.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.13.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.14.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.15.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.16.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.17.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.18.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.19.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.20.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.21.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.22.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  487: blocks.23.attn.rel_pos_t reshaped from torch.Size([31, 96]) to torch.Size([15, 96])
[07/13 10:06:43][INFO] checkpoint.py:  542: Network weights head.projection.weight not loaded.
[07/13 10:06:43][INFO] checkpoint.py:  542: Network weights head.projection.bias not loaded.
[07/13 10:06:43][INFO] checkpoint.py:  545: Network weights head.projection.weight not used.
[07/13 10:06:43][INFO] checkpoint.py:  545: Network weights head.projection.bias not used.
missing keys: ['head.projection.weight', 'head.projection.bias']
unexpected keys: []
[07/13 10:06:44][INFO] kinetics.py:   93: Constructing Kinetics train...
[07/13 10:06:44][INFO] kinetics.py:  169: Constructing kinetics dataloader (size: 1927 skip_rows 0) from train.csv 
[07/13 10:06:44][INFO] kinetics.py:   93: Constructing Kinetics val...
[07/13 10:06:44][INFO] kinetics.py:  169: Constructing kinetics dataloader (size: 482 skip_rows 0) from val.csv 
Using Prompting Method multi_cam_noisecropv2 with Params:
[07/13 10:06:44][INFO] train_net.py:  790: Start epoch: 1
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
[07/13 10:06:56][INFO] distributed.py: 1027: Reducer buckets have been rebuilt in this iteration.
[07/13 10:07:07][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.29190, "dt_data": 0.00042, "dt_net": 1.29148, "epoch": "1/200", "eta": "8:36:32", "gpu_mem": "17.91G", "grad_norm": 4.44934, "iter": "10/120", "loss": 2.80275, "lr": 0.0000024950, "top1_acc": 3.12500, "top1_err": 96.87500, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:07:20][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.31726, "dt_data": 0.00042, "dt_net": 1.31684, "epoch": "1/200", "eta": "8:46:27", "gpu_mem": "17.91G", "grad_norm": 3.80907, "iter": "20/120", "loss": 2.78455, "lr": 0.0000030450, "top1_acc": 3.12500, "top1_err": 96.87500, "top5_acc": 37.50000, "top5_err": 62.50000}
[07/13 10:07:34][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.32664, "dt_data": 0.00037, "dt_net": 1.32626, "epoch": "1/200", "eta": "8:49:59", "gpu_mem": "17.91G", "grad_norm": 4.66303, "iter": "30/120", "loss": 2.81473, "lr": 0.0000035950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 25.00000, "top5_err": 75.00000}
[07/13 10:07:47][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34026, "dt_data": 0.00044, "dt_net": 1.33982, "epoch": "1/200", "eta": "8:55:12", "gpu_mem": "17.91G", "grad_norm": 3.71385, "iter": "40/120", "loss": 2.80006, "lr": 0.0000041450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:08:00][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34377, "dt_data": 0.00044, "dt_net": 1.34332, "epoch": "1/200", "eta": "8:56:23", "gpu_mem": "17.91G", "grad_norm": 4.05713, "iter": "50/120", "loss": 2.78025, "lr": 0.0000046950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:08:14][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34297, "dt_data": 0.00043, "dt_net": 1.34255, "epoch": "1/200", "eta": "8:55:50", "gpu_mem": "17.91G", "grad_norm": 4.13480, "iter": "60/120", "loss": 2.76008, "lr": 0.0000052450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 40.62500, "top5_err": 59.37500}
[07/13 10:08:27][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.33084, "dt_data": 0.00048, "dt_net": 1.33037, "epoch": "1/200", "eta": "8:50:47", "gpu_mem": "17.91G", "grad_norm": 3.68994, "iter": "70/120", "loss": 2.79045, "lr": 0.0000057950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:08:41][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.33741, "dt_data": 0.00044, "dt_net": 1.33697, "epoch": "1/200", "eta": "8:53:10", "gpu_mem": "17.91G", "grad_norm": 3.91472, "iter": "80/120", "loss": 2.82656, "lr": 0.0000063450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 28.12500, "top5_err": 71.87500}
[07/13 10:08:54][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.33123, "dt_data": 0.00042, "dt_net": 1.33081, "epoch": "1/200", "eta": "8:50:29", "gpu_mem": "17.91G", "grad_norm": 4.09437, "iter": "90/120", "loss": 2.79109, "lr": 0.0000068950, "top1_acc": 0.00000, "top1_err": 100.00000, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:09:08][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35670, "dt_data": 0.00083, "dt_net": 1.35586, "epoch": "1/200", "eta": "9:00:25", "gpu_mem": "17.91G", "grad_norm": 4.54935, "iter": "100/120", "loss": 2.77316, "lr": 0.0000074450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 25.00000, "top5_err": 75.00000}
[07/13 10:09:21][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34764, "dt_data": 0.00042, "dt_net": 1.34722, "epoch": "1/200", "eta": "8:56:35", "gpu_mem": "17.91G", "grad_norm": 3.89386, "iter": "110/120", "loss": 2.77039, "lr": 0.0000079950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:09:35][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35411, "dt_data": 0.00028, "dt_net": 1.35382, "epoch": "1/200", "eta": "8:58:56", "gpu_mem": "17.91G", "grad_norm": 4.00834, "iter": "120/120", "loss": 2.82065, "lr": 0.0000085450, "top1_acc": 0.00000, "top1_err": 100.00000, "top5_acc": 28.12500, "top5_err": 71.87500}
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
[07/13 10:09:36][INFO] logging.py:  101: json_stats: {"RAM": "14.46/31.07G", "_type": "train_epoch", "dt": 0.80308, "dt_data": 0.80308, "dt_net": 1.35382, "epoch": "1/200", "eta": "5:19:36", "gpu_mem": "17.91G", "grad_norm": 4.00834, "loss": 2.79258, "lr": 0.0000085450, "top1_acc": 5.57292, "top1_err": 94.42708, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:09:36][INFO] train_net.py:  853: Epoch 0 takes 172.06s. Epochs from 0 to 0 take 172.06s in average and 172.06s in median.
[07/13 10:09:36][INFO] train_net.py:  859: For epoch 0, each iteraction takes 1.43s in average. From epoch 0 to 0, each iteraction takes 1.43s in average.
[07/13 10:10:00][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34151, "dt_data": 0.00052, "dt_net": 1.34098, "epoch": "2/200", "eta": "8:53:41", "gpu_mem": "17.91G", "grad_norm": 3.92982, "iter": "10/120", "loss": 2.76399, "lr": 0.0000090950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:10:13][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35577, "dt_data": 0.00064, "dt_net": 1.35513, "epoch": "2/200", "eta": "8:59:08", "gpu_mem": "17.91G", "grad_norm": 3.54674, "iter": "20/120", "loss": 2.74877, "lr": 0.0000096450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 46.87500, "top5_err": 53.12500}
[07/13 10:10:27][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34571, "dt_data": 0.00041, "dt_net": 1.34530, "epoch": "2/200", "eta": "8:54:55", "gpu_mem": "17.91G", "grad_norm": 4.27075, "iter": "30/120", "loss": 2.78940, "lr": 0.0000101950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 37.50000, "top5_err": 62.50000}
[07/13 10:10:40][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.36637, "dt_data": 0.00045, "dt_net": 1.36592, "epoch": "2/200", "eta": "9:02:54", "gpu_mem": "17.91G", "grad_norm": 4.00192, "iter": "40/120", "loss": 2.78827, "lr": 0.0000107450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 31.25000, "top5_err": 68.75000}
[07/13 10:10:54][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.39110, "dt_data": 0.00040, "dt_net": 1.39069, "epoch": "2/200", "eta": "9:12:29", "gpu_mem": "17.91G", "grad_norm": 4.20265, "iter": "50/120", "loss": 2.72398, "lr": 0.0000112950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 43.75000, "top5_err": 56.25000}
[07/13 10:11:07][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35022, "dt_data": 0.00042, "dt_net": 1.34980, "epoch": "2/200", "eta": "8:56:02", "gpu_mem": "17.91G", "grad_norm": 3.92501, "iter": "60/120", "loss": 2.74752, "lr": 0.0000118450, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 34.37500, "top5_err": 65.62500}
[07/13 10:11:21][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.37040, "dt_data": 0.00042, "dt_net": 1.36999, "epoch": "2/200", "eta": "9:03:49", "gpu_mem": "17.91G", "grad_norm": 3.61080, "iter": "70/120", "loss": 2.74010, "lr": 0.0000123950, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 40.62500, "top5_err": 59.37500}
[07/13 10:11:35][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35442, "dt_data": 0.00041, "dt_net": 1.35401, "epoch": "2/200", "eta": "8:57:15", "gpu_mem": "17.91G", "grad_norm": 4.11614, "iter": "80/120", "loss": 2.73297, "lr": 0.0000129450, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 43.75000, "top5_err": 56.25000}
[07/13 10:11:48][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.33556, "dt_data": 0.00041, "dt_net": 1.33515, "epoch": "2/200", "eta": "8:49:32", "gpu_mem": "17.91G", "grad_norm": 4.09910, "iter": "90/120", "loss": 2.73422, "lr": 0.0000134950, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 43.75000, "top5_err": 56.25000}
[07/13 10:12:02][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35653, "dt_data": 0.00043, "dt_net": 1.35610, "epoch": "2/200", "eta": "8:57:38", "gpu_mem": "17.91G", "grad_norm": 4.01988, "iter": "100/120", "loss": 2.70621, "lr": 0.0000140450, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 37.50000, "top5_err": 62.50000}
[07/13 10:12:15][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.36045, "dt_data": 0.00043, "dt_net": 1.36002, "epoch": "2/200", "eta": "8:58:57", "gpu_mem": "17.91G", "grad_norm": 4.78799, "iter": "110/120", "loss": 2.73838, "lr": 0.0000145950, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 37.50000, "top5_err": 62.50000}
[07/13 10:12:29][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.36109, "dt_data": 0.00028, "dt_net": 1.36081, "epoch": "2/200", "eta": "8:58:59", "gpu_mem": "17.91G", "grad_norm": 3.76392, "iter": "120/120", "loss": 2.73253, "lr": 0.0000151450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 40.62500, "top5_err": 59.37500}
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
[07/13 10:12:30][INFO] logging.py:  101: json_stats: {"RAM": "14.47/31.07G", "_type": "train_epoch", "dt": 0.79320, "dt_data": 0.79320, "dt_net": 1.36081, "epoch": "2/200", "eta": "5:14:05", "gpu_mem": "17.91G", "grad_norm": 3.76392, "loss": 2.74831, "lr": 0.0000151450, "top1_acc": 8.85417, "top1_err": 91.14583, "top5_acc": 39.01042, "top5_err": 60.98958}
[07/13 10:12:30][INFO] train_net.py:  853: Epoch 1 takes 174.19s. Epochs from 0 to 1 take 173.12s in average and 173.12s in median.
[07/13 10:12:30][INFO] train_net.py:  859: For epoch 1, each iteraction takes 1.45s in average. From epoch 0 to 1, each iteraction takes 1.44s in average.
[07/13 10:12:43][INFO] logging.py:  101: json_stats: {"_type": "val_iter", "epoch": "2/200", "eta": "0:00:06", "gpu_mem": "17.91G", "iter": "10/31", "time_diff": 0.32162, "top1_acc": 12.50000, "top1_err": 87.50000, "top5_acc": 53.12500, "top5_err": 46.87500}
[07/13 10:12:47][INFO] logging.py:  101: json_stats: {"_type": "val_iter", "epoch": "2/200", "eta": "0:00:03", "gpu_mem": "17.91G", "iter": "20/31", "time_diff": 0.34919, "top1_acc": 18.75000, "top1_err": 81.25000, "top5_acc": 50.00000, "top5_err": 50.00000}
[07/13 10:12:50][INFO] logging.py:  101: json_stats: {"_type": "val_iter", "epoch": "2/200", "eta": "0:00:00", "gpu_mem": "17.91G", "iter": "30/31", "time_diff": 0.31126, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 40.62500, "top5_err": 59.37500}
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
[07/13 10:12:51][INFO] logging.py:  101: json_stats: {"RAM": "14.47/31.07G", "_type": "val_epoch", "epoch": "2/200", "gpu_mem": "17.91G", "min_top1_err": 83.81743, "min_top5_err": 49.58506, "time_diff": 0.61150, "top1_acc": 16.18257, "top1_err": 83.81743, "top5_acc": 50.41494, "top5_err": 49.58506}
SKIPPING PROMPT OPTIMIZATION STEP
Val Top1 Accuracy History: [0]
[07/13 10:13:14][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34525, "dt_data": 0.00041, "dt_net": 1.34484, "epoch": "3/200", "eta": "8:52:29", "gpu_mem": "17.91G", "grad_norm": 4.26440, "iter": "10/120", "loss": 2.71918, "lr": 0.0000156950, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 46.87500, "top5_err": 53.12500}
[07/13 10:13:28][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34524, "dt_data": 0.00041, "dt_net": 1.34482, "epoch": "3/200", "eta": "8:52:15", "gpu_mem": "17.91G", "grad_norm": 3.85109, "iter": "20/120", "loss": 2.68931, "lr": 0.0000162450, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 50.00000, "top5_err": 50.00000}
[07/13 10:13:41][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.36021, "dt_data": 0.00092, "dt_net": 1.35929, "epoch": "3/200", "eta": "8:57:57", "gpu_mem": "17.91G", "grad_norm": 4.58615, "iter": "30/120", "loss": 2.69245, "lr": 0.0000167950, "top1_acc": 12.50000, "top1_err": 87.50000, "top5_acc": 46.87500, "top5_err": 53.12500}
[07/13 10:13:55][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.38912, "dt_data": 0.00042, "dt_net": 1.38870, "epoch": "3/200", "eta": "9:09:09", "gpu_mem": "17.91G", "grad_norm": 4.11883, "iter": "40/120", "loss": 2.72083, "lr": 0.0000173450, "top1_acc": 6.25000, "top1_err": 93.75000, "top5_acc": 43.75000, "top5_err": 56.25000}
[07/13 10:14:09][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35179, "dt_data": 0.00049, "dt_net": 1.35130, "epoch": "3/200", "eta": "8:54:10", "gpu_mem": "17.91G", "grad_norm": 4.14463, "iter": "50/120", "loss": 2.65113, "lr": 0.0000178950, "top1_acc": 18.75000, "top1_err": 81.25000, "top5_acc": 56.25000, "top5_err": 43.75000}
[07/13 10:14:22][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.39435, "dt_data": 0.00044, "dt_net": 1.39391, "epoch": "3/200", "eta": "9:10:46", "gpu_mem": "17.91G", "grad_norm": 4.92485, "iter": "60/120", "loss": 2.68609, "lr": 0.0000184450, "top1_acc": 9.37500, "top1_err": 90.62500, "top5_acc": 53.12500, "top5_err": 46.87500}
[07/13 10:14:36][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35014, "dt_data": 0.00046, "dt_net": 1.34968, "epoch": "3/200", "eta": "8:53:04", "gpu_mem": "17.91G", "grad_norm": 4.55523, "iter": "70/120", "loss": 2.69764, "lr": 0.0000189950, "top1_acc": 12.50000, "top1_err": 87.50000, "top5_acc": 50.00000, "top5_err": 50.00000}
[07/13 10:14:50][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.33863, "dt_data": 0.00043, "dt_net": 1.33819, "epoch": "3/200", "eta": "8:48:18", "gpu_mem": "17.91G", "grad_norm": 4.41579, "iter": "80/120", "loss": 2.66059, "lr": 0.0000195450, "top1_acc": 12.50000, "top1_err": 87.50000, "top5_acc": 62.50000, "top5_err": 37.50000}
[07/13 10:15:03][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.36489, "dt_data": 0.00045, "dt_net": 1.36444, "epoch": "3/200", "eta": "8:58:27", "gpu_mem": "17.91G", "grad_norm": 4.21990, "iter": "90/120", "loss": 2.65932, "lr": 0.0000200950, "top1_acc": 18.75000, "top1_err": 81.25000, "top5_acc": 56.25000, "top5_err": 43.75000}
[07/13 10:15:17][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.35894, "dt_data": 0.00052, "dt_net": 1.35842, "epoch": "3/200", "eta": "8:55:52", "gpu_mem": "17.91G", "grad_norm": 3.97860, "iter": "100/120", "loss": 2.63842, "lr": 0.0000206450, "top1_acc": 21.87500, "top1_err": 78.12500, "top5_acc": 56.25000, "top5_err": 43.75000}
[07/13 10:15:30][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.37922, "dt_data": 0.00044, "dt_net": 1.37878, "epoch": "3/200", "eta": "9:03:38", "gpu_mem": "17.91G", "grad_norm": 4.52559, "iter": "110/120", "loss": 2.60011, "lr": 0.0000211950, "top1_acc": 18.75000, "top1_err": 81.25000, "top5_acc": 56.25000, "top5_err": 43.75000}
[07/13 10:15:44][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.34495, "dt_data": 0.00030, "dt_net": 1.34465, "epoch": "3/200", "eta": "8:49:54", "gpu_mem": "17.91G", "grad_norm": 4.29386, "iter": "120/120", "loss": 2.58771, "lr": 0.0000217450, "top1_acc": 21.87500, "top1_err": 78.12500, "top5_acc": 68.75000, "top5_err": 31.25000}
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_functional_video.py:6: UserWarning: The 'torchvision.transforms._functional_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms.functional' module instead.
  warnings.warn(
/usr/local/lib/python3.8/dist-packages/torchvision-0.14.1a0+5e8e2f1-py3.8-linux-x86_64.egg/torchvision/transforms/_transforms_video.py:22: UserWarning: The 'torchvision.transforms._transforms_video' module is deprecated since 0.12 and will be removed in the future. Please use the 'torchvision.transforms' module instead.
  warnings.warn(
[07/13 10:15:45][INFO] logging.py:  101: json_stats: {"RAM": "14.34/31.07G", "_type": "train_epoch", "dt": 0.80217, "dt_data": 0.80217, "dt_net": 1.34465, "epoch": "3/200", "eta": "5:16:02", "gpu_mem": "17.91G", "grad_norm": 4.29386, "loss": 2.66250, "lr": 0.0000217450, "top1_acc": 15.41667, "top1_err": 84.58333, "top5_acc": 53.59375, "top5_err": 46.40625}
[07/13 10:15:45][INFO] train_net.py:  853: Epoch 2 takes 174.08s. Epochs from 0 to 2 take 173.44s in average and 174.08s in median.
[07/13 10:15:45][INFO] train_net.py:  859: For epoch 2, each iteraction takes 1.45s in average. From epoch 0 to 2, each iteraction takes 1.45s in average.
[07/13 10:16:08][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.36464, "dt_data": 0.00044, "dt_net": 1.36420, "epoch": "4/200", "eta": "8:57:26", "gpu_mem": "17.91G", "grad_norm": 4.16927, "iter": "10/120", "loss": 2.60279, "lr": 0.0000222950, "top1_acc": 25.00000, "top1_err": 75.00000, "top5_acc": 59.37500, "top5_err": 40.62500}
[07/13 10:16:22][INFO] logging.py:  101: json_stats: {"_type": "train_iter_", "dt": 1.39682, "dt_data": 0.00043, "dt_net": 1.39639, "epoch": "4/200", "eta": "9:09:52", "gpu_mem": "17.91G", "grad_norm": 4.70956, "iter": "20/120", "loss": 2.54258, "lr": 0.0000228450, "top1_acc": 21.87500, "top1_err": 78.12500, "top5_acc": 62.50000, "top5_err": 37.50000}
